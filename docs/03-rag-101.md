# RAG 101: Retrieval-Augmented Generation

Руководство по идее RAG, по шагам пайплайна и по практическим рекомендациям. Применимо к проекту RAG HH и к любым системам «поиск + генерация ответа».

---

## 1. Что такое RAG

**RAG (Retrieval-Augmented Generation)** — это подход к ответам на вопросы по вашей коллекции документов:

1. **Retrieval** — по запросу пользователя находим релевантные фрагменты (документы, чанки) с помощью поиска, чаще всего семантического (по эмбеддингам).
2. **Augmented** — найденные фрагменты подставляются в запрос к языковой модели как **контекст**.
3. **Generation** — модель генерирует ответ, опираясь на этот контекст, а не только на «знания из обучения».

Зачем это нужно:

- Модель не знает ваши внутренние документы или актуальные данные (вакансии, база знаний, документация).
- Прямая подстановка контекста в промпт даёт модели «факты» и снижает галлюцинации по теме.
- Обновление ответов сводится к обновлению хранилища документов и индекса, без переобучения модели.

---

## 2. Упрощённая схема пайплайна

```
[Запрос пользователя]
        │
        ▼
[Эмбеддинг запроса]  ──►  [Векторный поиск по БД]  ──►  [Топ-N документов/чанков]
        │                                                        │
        │                                                        ▼
        │                                            [Формирование контекста]
        │                                                        │
        └────────────────────────────────────────────────────────┤
                                                                 ▼
                                              [Промпт: запрос + контекст]
                                                                 │
                                                                 ▼
                                              [LLM генерирует ответ]
                                                                 │
                                                                 ▼
                                              [Ответ пользователю]
```

В RAG HH мы реализуем всё до шага «контекст готов»; вызов LLM можно добавить отдельно (см. раздел 6).

---

## 3. Retrieval: что и как ищем

### 3.1 Семантический поиск

- Запрос и документы превращаются в векторы **одной и той же моделью**.
- Ищем k ближайших векторов (в нашем случае по косинусной близости в pgvector).
- Это даёт «похоже по смыслу», а не только совпадение слов.

### 3.2 Гибридный поиск (best practice)

Часто комбинируют:

- **Семантический** (векторы) — смысл.
- **Ключевые слова** (full-text search, BM25) — точные термины, названия, коды.

Скоры от двух поисков объединяют (например, взвешенная сумма или RRF). В текущем RAG HH только семантика; при желании можно добавить полнотекстовый поиск PostgreSQL (`tsvector`/`tsquery`) по полям вакансий.

### 3.3 Сколько документов возвращать (top-k)

- Слишком мало — модель не видит нужный факт.
- Слишком много — контекст не влезает в окно модели и зашумляет ответ.

Типичный диапазон: **3–10** документов (или чанков). В API RAG HH параметр `limit` по умолчанию 5 — это и есть top-k для контекста.

---

## 4. Chunking: когда документы большие

Если один «документ» (например, длинное описание вакансии) больше, чем эффективное окно эмбеддинг-модели (часто 512 токенов):

- Документ **разбивают на чанки** (абзацы, фиксированный размер в символах/токенах с перекрытием).
- Эмбеддинг считают для **каждого чанка**; в БД хранят чанки и ссылку на родительский документ.
- При поиске возвращаются чанки; контекст для LLM собирают из чанков (и при необходимости из родительских документов).

В RAG HH вакансия — один объект среднего размера, описание обрезается до 3000 символов и эмбеддится целиком. Для очень длинных документов в других проектах применяют именно chunking.

---

## 5. Формирование контекста

- Из топ-N результатов нужно собрать один текст для промпта: например, склеить заголовок + фрагмент с разделителями.
- Важно явно помечать границы документов («[Вакансия 1] …», «[Вакансия 2] …»), чтобы модель не смешивала источники.
- Укладываться в лимит токенов модели (контекстное окно). При необходимости — обрезать самые слабо релевантные или сокращать чанки.

В проекте эндпоинт **GET /rag** как раз возвращает готовый `context` (и список `sources` с ссылками). Этот `context` можно вставить в промпт для любой LLM.

---

## 6. Generation: вызов LLM (опционально в RAG HH)

Сейчас RAG HH **не вызывает LLM** — только отдаёт контекст и источники. Чтобы замкнуть RAG:

1. Взять из ответа `/rag` поля `query` и `context`.
2. Сформировать промпт, например:
   - «По следующим вакансиям ответь на вопрос. Вакансии: {context}. Вопрос: {query}. Ответ:»
3. Отправить промпт в выбранную LLM (OpenAI, локальная Ollama, и т.д.).
4. Вернуть пользователю сгенерированный ответ и при желании — `sources` из `/rag`.

Best practice: в промпте явно просить отвечать только по контексту и не выдумывать вакансии.

---

## 7. Best practices (кратко)

- **Одна модель эмбеддингов** для индекса и запросов; размерность и тип колонки в БД должны совпадать.
- **Top-k** подбирать по качеству ответов (3–10 — хороший старт).
- **Ограничивать размер контекста** под лимит контекстного окна LLM.
- **Чётко размечать документы в контексте** ([Документ 1], [Документ 2], …).
- При длинных документах — **chunking** и эмбеддинг чанков; при необходимости — гибрид с full-text поиском.
- **Оценка**: тестовые вопросы, сравнение ответов с эталоном или проверка наличия нужного источника в контексте.

---

## 8. Где в проекте RAG HH

- **Retrieval**: `GET /search` и внутренняя функция `search_similar()` — семантический поиск по pgvector.
- **Формирование контекста для RAG**: `GET /rag` — тот же поиск + сборка строки `context` и списка `sources`.
- **Generation**: не реализован; контекст из `/rag` готов к подстановке в любой LLM по вашему коду.

Дальше: [Архитектура проекта](04-project-architecture.md) — как эти блоки собраны в приложении и API.
