# Эмбеддинги 101: от идеи до выбора модели

Это руководство объясняет, что такое эмбеддинги, зачем они нужны в RAG и векторном поиске, и как с ними работать в контексте проекта RAG HH.

---

## 1. Что такое эмбеддинг

**Эмбеддинг (embedding)** — это числовой вектор фиксированной длины, который кодирует **смысл** текста (или запроса). Один и тот же текст при одном и том же модели всегда даёт один и тот же вектор (в одном режиме инференса).

- **Размерность** — сколько чисел в векторе (например, 384, 768, 1536). Зависит от модели.
- **Смысловая близость**: тексты с похожим смыслом дают векторы, которые «близко» в пространстве (по косинусному расстоянию или евклидову). Разный смысл — векторы дальше.

Так мы переводим задачу «найти похожие по смыслу тексты» в задачу **поиска ближайших векторов** (k-NN), которую умеет решать pgvector и другие векторные БД.

---

## 2. Зачем эмбеддинги в нашем проекте

- **Индексация**: каждая вакансия превращается в текст (название + описание + навыки), текст — в вектор. Вектор храним в PostgreSQL рядом с полями вакансии.
- **Поиск**: запрос пользователя («удалённая работа python») тоже превращаем в вектор тем же моделью. Ищем в БД вакансии с **ближайшими** векторами — это и есть семантический поиск.
- **RAG**: те же ближайшие вакансии отдаём как контекст в LLM для генерации ответа.

Важно: и документы, и запросы должны эмбеддиться **одной и той же моделью** и в одной размерности, иначе расстояния между векторами теряют смысл.

---

## 3. Модели эмбеддингов: что есть на рынке

### 3.1 Локальные (sentence-transformers и аналоги)

- Работают на вашем железе (CPU или GPU).
- Нет платы за запрос, данные не уходят наружу.
- В проекте используется **sentence-transformers** и модель **paraphrase-multilingual-MiniLM-L12-v2** (поддержка русского и английского, размерность 384).

**Плюсы**: бесплатно, предсказуемая задержка, приватность.  
**Минусы**: нужны ресурсы (RAM, при больших объёмах — GPU), ограничение по длине текста (обычно до 512 токенов у многих моделей).

### 3.2 Облачные API (OpenAI, Cohere, и др.)

- Текст отправляется в API, возвращается вектор (например, 1536 для `text-embedding-3-small`).
- Плюсы: не нужно поднимать модель, часто высокая качество и многоязычность.  
- Минусы: стоимость, задержка сети, необходимость хранить ключи и учитывать лимиты.

Для pet-проекта и обучения удобно начать с локальной модели; для продакшена можно рассмотреть гибрид (локально для индексации, API для запросов) или полностью API.

---

## 4. Размерность вектора

- **Размерность** задаётся моделью (384 у MiniLM-L12, 768 у многих BERT-подобных, 1536 у части OpenAI).
- В PostgreSQL тип `vector(384)` означает ровно 384 float. Менять модель — значит менять размерность колонки и пересчитывать эмбеддинги.
- Большая размерность не всегда «лучше»: больше памяти и времени на поиск. Для многих задач 384–768 достаточно.

В RAG HH используется **384** — это соответствует модели `paraphrase-multilingual-MiniLM-L12-v2` и колонке `embedding vector(384)` в БД.

---

## 5. Длина текста и обрезка

У моделей есть **максимальная длина в токенах** (часто 128–512). Текст длиннее обрезается или обрабатывается по частям.

В проекте описание вакансии обрезается до 3000 символов (`vacancy_to_text` в `hh_client.py`), что укладывается в лимит типичной модели. Для очень длинных документов в RAG принято **разбивать на чанки** и эмбеддить чанки, а не весь документ целиком (подробнее в RAG 101).

---

## 6. Best practices

1. **Одна модель везде** — и для индексации, и для запросов. Смена модели требует пересчёта всех эмбеддингов и смены типа колонки в БД.
2. **Нормализация текста** — убирать лишний HTML, лишние пробелы, опционально приводить к одному регистру для языков, где это уместно. В проекте HTML из описаний вычищается перед эмбеддингом.
3. **Пакетная обработка** — для индексации вызывать `embed_batch(texts, batch_size=32)` вместо по одному `embed(text)` — быстрее и меньше нагрузка на модель.
4. **Кэширование модели** — загружать модель один раз (в проекте — через `@lru_cache` в `get_embedding_model()`), а не при каждом запросе.
5. **Язык** — для русскоязычных запросов и текстов использовать multilingual-модель (как в проекте).

---

## 7. Где в проекте RAG HH

- Загрузка модели и расчёт векторов: `app/embeddings.py` (`embed`, `embed_batch`).
- Конфиг модели: `EMBEDDING_MODEL` в `app/config.py` и переменная окружения.
- Подготовка текста вакансии перед эмбеддингом: `vacancy_to_text()` в `app/hh_client.py`.
- Запись вектора в БД: `app/vacancies.py` (колонка `embedding vector(384)`).

После прочтения этого гайда логично перейти к [pgvector 101](02-pgvector-101.md) — как эти векторы хранить и искать в PostgreSQL.
